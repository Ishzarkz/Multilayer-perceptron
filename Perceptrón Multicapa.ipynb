{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c0851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_images import get_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d4f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = './mnist_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfd466",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_num, y_train_num, x_test_num, y_test_num = get_images(mnist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a48c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_num[:50000].reshape(50000, -1).astype(np.float32)/255\n",
    "y_train = y_train_num[:50000].reshape(50000, 1)\n",
    "\n",
    "x_val = x_train_num[50000:].reshape(10000, -1).astype(float)/255\n",
    "y_val = y_train_num[50000:].reshape(10000, 1)\n",
    "\n",
    "x_test = x_test_num.copy().reshape(10000, -1).astype(float)/255\n",
    "y_test = y_test_num.copy().reshape(10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af786e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    \n",
    "    assert x.shape[0] == y.shape[0], 'Error Muestras'\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]\n",
    "        \n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c0e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parametros de inicialización\n",
    "def init_parameters(input_size, neurons):\n",
    "    \n",
    "    W1 = np.random.randn(neurons[0], input_size) * 0.001\n",
    "    b1 = np.zeros((neurons[0], 1))\n",
    "    \n",
    "    W2 = np.random.randn(neurons[1], neurons[0]) * 0.756015625\n",
    "    b2 = np.zeros((neurons[1], 1))\n",
    "    \n",
    "    W3 = np.random.randn(neurons[2], neurons[1]) * 0.758359375\n",
    "    b3 = np.zeros((neurons[2], 1))\n",
    "    \n",
    "    W4 = np.random.randn(neurons[3], neurons[2]) * 0.618125\n",
    "    b4 = np.zeros((neurons[3], 1))\n",
    "    \n",
    "    W5 = np.random.randn(neurons[4], neurons[3]) * 0.330996094\n",
    "    b5 = np.zeros((neurons[4], 1))\n",
    "    \n",
    "    W6 = np.random.randn(neurons[5], neurons[4]) * 0.121914063\n",
    "    b6 = np.zeros((neurons[5], 1))\n",
    "    \n",
    "    W7 = np.random.randn(neurons[6], neurons[5]) * 0.19\n",
    "    b7 = np.zeros((neurons[6], 1))\n",
    "    \n",
    "    W8 = np.random.randn(neurons[7], neurons[6]) * 0.001\n",
    "    b8 = np.zeros((neurons[7], 1))\n",
    "    \n",
    "    return {'W1':W1, 'b1':b1, 'W2':W2, 'b2':b2, 'W3':W3, 'b3':b3, 'W4':W4, 'b4':b4, 'W5':W5, 'b5':b5, 'W6':W6, 'b6':b6, 'W7':W7, 'b7':b7, 'W8':W8, 'b8':b8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a64d09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Revisar forma de las capas de neuronas y cambiar tamaños\n",
    "parameters = init_parameters(28*28, [100,100,100,100,100,100,100,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14530e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c342455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores1(x, parameters, activation_fcn):\n",
    "    \n",
    "    z1 = parameters['W1'] @ x + parameters['b1']\n",
    "    a1 = activation_fcn(z1)\n",
    "    \n",
    "    z2 = parameters['W2'] @ a1 + parameters['b2']\n",
    "    a2 = activation_fcn(z2)\n",
    "    \n",
    "    z3 = parameters['W3'] @ a2 + parameters['b3']\n",
    "    a3 = activation_fcn(z3)\n",
    "    \n",
    "    z4 = parameters['W4'] @ a3 + parameters['b4']\n",
    "    a4 = activation_fcn(z4)\n",
    "    \n",
    "    z5 = parameters['W5'] @ a4 + parameters['b5']\n",
    "    a5 = activation_fcn(z5)\n",
    "    \n",
    "    z6 = parameters['W6'] @ a5 + parameters['b6']\n",
    "    a6 = activation_fcn(z6)\n",
    "    \n",
    "    z7 = parameters['W7'] @ a6 + parameters['b7']\n",
    "    a7 = activation_fcn(z6)\n",
    "    \n",
    "    z8 = parameters['W8'] @ a7 + parameters['b8']\n",
    "    \n",
    "    return z8, z7, z6, z5, z4, z3, z2, z1, a1, a2, a3, a4, a5, a6, a7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, z7, z6, z5, z4, z3, z2, z1, a1, a2, a3, a4, a5, a6, a7 = scores1(x_train[:64].T, parameters, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ccdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    sum_exp_scores = np.sum(exp_scores, axis=0)\n",
    "    probs = exp_scores/sum_exp_scores\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_entropy(scores, y, batch_size=64):\n",
    "    \n",
    "    probs = softmax(scores)\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "    \n",
    "    return probs, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(probs, x, y, z7, z6, z5, z4, z3, z2, z1, a1, a2, a3, a4, a5, a6, a7 , scores, parameters, batch_size=64):\n",
    "    \n",
    "    grads = {}\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1\n",
    "        \n",
    "    dz8 = probs.copy()\n",
    "    dW8 = dz8 @ a7.T / batch_size\n",
    "    db8 = np.sum(dz8, axis=1, keepdims=True)  / batch_size\n",
    "    \n",
    "    da7 = parameters['W8'].T @ dz8\n",
    "    dz7 = da7.copy()\n",
    "    dz7 [z7 <= 0 ] = 0\n",
    "    dW7 = dz7 @ a6.T / batch_size\n",
    "    db7 = np.sum(dz7, axis=1, keepdims=True)  / batch_size\n",
    "    \n",
    "    da6 = parameters['W7'].T @ dz7\n",
    "    dz6 = da6.copy()\n",
    "    dz6 [z6 <= 0 ] = 0\n",
    "    dW6 = dz6 @ a5.T / batch_size\n",
    "    db6 = np.sum(dz6, axis=1, keepdims=True)  / batch_size\n",
    "    \n",
    "    da5 = parameters['W6'].T @ dz6\n",
    "    dz5 = da5.copy()\n",
    "    dz5 [z5 <= 0 ] = 0\n",
    "    dW5 = dz5 @ a4.T / batch_size\n",
    "    db5 = np.sum(dz5, axis=1, keepdims=True) / batch_size\n",
    "    \n",
    "    da4 = parameters['W5'].T @ dz5\n",
    "    dz4 = da4.copy()\n",
    "    dz4 [z4 <= 0 ] = 0\n",
    "    dW4 = dz4 @ a3.T / batch_size\n",
    "    db4 = np.sum(dz4, axis=1, keepdims=True)/ batch_size\n",
    "    \n",
    "    da3 = parameters['W4'].T @ dz4 \n",
    "    dz3 = da3.copy()\n",
    "    dz3 [z3 <= 0 ] = 0\n",
    "    dW3 = dz3 @ a2.T / batch_size\n",
    "    db3 = np.sum(dz3, axis=1, keepdims=True) / batch_size\n",
    "    \n",
    "    da2 = parameters['W3'].T @ dz3\n",
    "    dz2 = da2.copy()\n",
    "    dz2 [z2 <= 0 ] = 0\n",
    "    dW2 = dz2 @ a1.T / batch_size\n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True) / batch_size\n",
    "    \n",
    "    da1 = parameters['W2'].T @ dz2\n",
    "    dz1 = da1.copy()\n",
    "    dz1 [z1 <= 0 ] = 0\n",
    "    dW1 = dz1 @ x\n",
    "    db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    assert parameters['W1'].shape == dW1.shape, 'Forma diferente W1'\n",
    "    assert parameters['W2'].shape == dW2.shape, 'Forma diferente W2'\n",
    "    assert parameters['W3'].shape == dW3.shape, 'Forma diferente W3'\n",
    "    assert parameters['W4'].shape == dW4.shape, 'Forma diferente W4'\n",
    "    assert parameters['W5'].shape == dW5.shape, 'Forma diferente W5'\n",
    "    assert parameters['W6'].shape == dW6.shape, 'Forma diferente W6'\n",
    "    assert parameters['W7'].shape == dW7.shape, 'Forma diferente W7'\n",
    "    assert parameters['W8'].shape == dW8.shape, 'Forma diferente W8'\n",
    "    \n",
    "    assert parameters['b1'].shape == db1.shape, 'Forma diferente b1'\n",
    "    assert parameters['b2'].shape == db2.shape, 'Forma diferente b2'\n",
    "    assert parameters['b3'].shape == db3.shape, 'Forma diferente b3'\n",
    "    assert parameters['b4'].shape == db4.shape, 'Forma diferente b4'\n",
    "    assert parameters['b5'].shape == db5.shape, 'Forma diferente b5'\n",
    "    assert parameters['b6'].shape == db6.shape, 'Forma diferente b6'\n",
    "    assert parameters['b7'].shape == db7.shape, 'Forma diferente b7'\n",
    "    assert parameters['b8'].shape == db8.shape, 'Forma diferente b8'\n",
    "    \n",
    "    \n",
    "    grads = {'w1':dW1,  'b1':db1, 'w2':dW2, 'b2':db2, 'w3':dW3,  'b3':db3, 'w4':dW4, 'b4':db4, 'w5':dW5,  'b5':db5, 'w6':dW6, 'b6':db6, 'w7':dW7, 'b7':db7, 'w8':dW8, 'b8':db8}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e4903",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pruebas del calculo de gradientes\n",
    "y_hat, cost = x_entropy(scores, y_train[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Revisar formas\n",
    "#print(y_hat.shape)\n",
    "#print(x_train.T.shape)\n",
    "#print(scores.shape)\n",
    "#print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88caf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = backward(y_hat, x_train[:64], y_train[:64], z7, z6, z5, z4, z3, z2, z1, a1, a2, a3, a4, a5, a6, a7, scores, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x_data, y_data, mb_size=64):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, x_data, y_data)):\n",
    "        scores2, z7, z6, z5, z4, z3, z2, z1, a1, a2, a3, a4, a5, a6, a7 = scores1(x.T, parameters, relu)\n",
    "        y_hat, cost = x_entropy(scores2, y, batch_size=len(x))\n",
    "        \n",
    "        correct += np.sum(np.argmax(y_hat, axis=0) == y.squeeze())\n",
    "        total += y_hat.shape[1]\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755c7c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, parameters, mb_size=64, learning_rate = 1e-4):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, x_train, y_train)):\n",
    "            scores2, z7, z6, z5, z4, z3, z2, z1, a1, a2, a3, a4, a5, a6, a7 = scores1(x.T, parameters=parameters, activation_fcn=relu)\n",
    "            y_hat, cost = x_entropy(scores2, y, batch_size=len(x))\n",
    "            grads = backward(y_hat, x, y, z7, z6, z5, z4, z3, z2, z1, a1, a2, a3, a4, a5, a6, a7, scores2, parameters, batch_size=len(x))\n",
    "            \n",
    "            parameters['W1'] = parameters['W1'] - learning_rate*grads['w1']\n",
    "            parameters['b1'] = parameters['b1'] - learning_rate*grads['b1']\n",
    "            parameters['b2'] = parameters['b2'] - learning_rate*grads['b2']\n",
    "            parameters['W2'] = parameters['W2'] - learning_rate*grads['w2']\n",
    "            parameters['W3'] = parameters['W3'] - learning_rate*grads['w3']\n",
    "            parameters['b3'] = parameters['b3'] - learning_rate*grads['b3']\n",
    "            parameters['b4'] = parameters['b4'] - learning_rate*grads['b4']\n",
    "            parameters['W4'] = parameters['W4'] - learning_rate*grads['w4']\n",
    "            parameters['W5'] = parameters['W5'] - learning_rate*grads['w5']\n",
    "            parameters['b5'] = parameters['b5'] - learning_rate*grads['b5']\n",
    "            parameters['b6'] = parameters['b6'] - learning_rate*grads['b6']\n",
    "            parameters['W6'] = parameters['W6'] - learning_rate*grads['w6']\n",
    "            parameters['W7'] = parameters['W7'] - learning_rate*grads['w7']\n",
    "            parameters['b7'] = parameters['b7'] - learning_rate*grads['b7']\n",
    "            parameters['b8'] = parameters['b8'] - learning_rate*grads['b8']\n",
    "            parameters['W8'] = parameters['W8'] - learning_rate*grads['w8']\n",
    "            \n",
    "        print(f'costo: {cost} precisión: {accuracy(x_val, y_val, mb_size)} precisión_entrenamiento: {accuracy(x_train, y_train, mb_size)} precisión_prueba: {accuracy(x_test, y_test, mb_size)}')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b4d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Parametros para el entrenamiento de la red neuronal\n",
    "mb_size = 1024\n",
    "learning_rate = 1e-4\n",
    "epochs = 400\n",
    "parameters = train(epochs=epochs,parameters=parameters,mb_size=mb_size,learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(accuracy(x_train, y_train, mb_size))\n",
    "#print(accuracy(x_test, y_test, mb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e420776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    scores2,_,_,_,_,_,_,_,_,_,_,_,_,_,_= scores1(x, parameters, relu)\n",
    "    return np.argmax(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0].reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number(image):\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_number(x_test_num[idx])\n",
    "pred = predict(x_test[idx].reshape(-1, 1))\n",
    "print(f'Valor predicho: {pred}. Valor real:{y_test[idx][0]}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
